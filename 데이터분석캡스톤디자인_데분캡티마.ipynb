{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> 데이터분석캡스톤디자인 </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> 소셜 데이터를 활용한 텍스트 분석: 인스타그램 기반 검색 키워드 관련 트렌드 제공 </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align: right\"> 2022년 1학기 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align: right\"> 지도 교수: 이원희 교수님 </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align: right\"> 팀 C: 2018100900 모준우, 2020103944 온유나, 2018100922 이승건 </div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 필요 모듈 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import TfidfModel\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import FreqDist\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import re\n",
    "from selenium import webdriver\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "import smart_open\n",
    "import time\n",
    "import warnings\n",
    "from wordcloud import WordCloud \n",
    "from wordcloud import STOPWORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수 정의: 한글인지, 아닌지 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isKoreanIncluded(word):\n",
    "        for i in word:\n",
    "            if ord(i) > int('0x1100',16) and ord(i) < int('0x11ff',16) :\n",
    "                return True\n",
    "            if ord(i) > int('0x3131',16) and ord(i) < int('0x318e',16) :\n",
    "                return True\n",
    "            if ord(i) > int('0xa960',16) and ord(i) < int('0xa97c',16) :\n",
    "                return True\n",
    "            if ord(i) > int('0xac00',16) and ord(i) < int('0xd7a3',16) :\n",
    "                return True\n",
    "            if ord(i) > int('0xd7b0',16) and ord(i) < int('0xd7fb',16) :\n",
    "                return True\n",
    "\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "# Crawling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수 정의: 검색 키워드의 페이지를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insta_searching(word):\n",
    "    url = \"https://www.instagram.com/explore/tags/\" + str(word)\n",
    "    return url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수 정의: 검색 페이지에서 첫번째 게시글을 클릭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_first(driver):\n",
    "    first = driver.find_elements_by_css_selector(\"div._aagw\")[0]\n",
    "    print(type(first))\n",
    "    first.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수 정의: 게시글 내용, 게시글 작성 일시, 좋아요 수, 해시태그 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content(driver):\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    try:\n",
    "        content = soup.select('div._a9zs > span')[0].text\n",
    "    except:\n",
    "        content = ' '\n",
    "    \n",
    "    tags = re.findall(r'#[^\\s#,\\\\]+', content)\n",
    "    \n",
    "    date = soup.select('time._aaqe')[0]['datetime'][:10]\n",
    "    \n",
    "    try:\n",
    "        like = soup.select('div._aacl._aaco._aacw._aacx._aada._aade > span')\n",
    "    except:\n",
    "        like = 0\n",
    "    \n",
    "    try:\n",
    "        place =soup.select('div._aacl._aacn._aacu._aacy._aada._aade')[0].text\n",
    "    except:\n",
    "        place = ''\n",
    "    \n",
    "    data = [content, date, like, place, tags]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수 정의: 다음 게시물로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move_next(driver):\n",
    "    right = driver.find_element_by_css_selector(\"div._aaqg._aaqh\")\n",
    "    right.click()\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### chrome driver 작동\n",
    "\n",
    "* 코드가 실행되는 컴퓨터의 chrome 버전에 맞는 chrome driver를 다운받아 코드와 같은 경로에 저장해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver =  webdriver.Chrome('chromedriver.exe')\n",
    "\n",
    "driver.get('https://www.instagram.com')\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 로그인\n",
    "\n",
    "* 개인의 인스타그램 아이디를 email 변수에, 비밀번호를 password 변수에 입력해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email = 'type_your_id' #'instagram ID'\n",
    "input_id = driver.find_elements_by_css_selector('input._2hvTZ.pexuQ.zyHYP')[0]\n",
    "input_id.clear()\n",
    "input_id.send_keys(email)\n",
    "\n",
    "password = 'type_your_password' #'instagram PW'\n",
    "input_pw = driver.find_elements_by_css_selector('input._2hvTZ.pexuQ.zyHYP')[1]\n",
    "input_pw.clear()\n",
    "input_pw.send_keys(password)\n",
    "input_pw.submit()\n",
    "\n",
    "time.sleep(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 제공받을 트렌드의 키워드(검색어)를 입력\n",
    "\n",
    "* 한글 검색어만 입력 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = input(\"한글 검색어를 입력하세요 : \")\n",
    "while True:\n",
    "    if isKoreanIncluded(word) == True:\n",
    "        search_word = str(word)\n",
    "        break\n",
    "        \n",
    "    else:\n",
    "        word = input(\"한글이 아닙니다. 한글 검색어를 입력하세요 : \")\n",
    "        \n",
    "url = insta_searching(search_word)        \n",
    "driver.get(url)\n",
    "time.sleep(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자가 입력한 검색 키워드의 검색 키워드를 찾을 수 있다면 크롤링 시작\n",
    "* 찾을 수 없을 땐 새로운 검색어를 입력해야 함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    if driver.find_elements_by_css_selector(\"div._aagw\") == []:\n",
    "        print(\"죄송합니다. 검색 결과를 찾을 수 없습니다.\")\n",
    "        word = input(\"새로운 검색어를 입력하세요 : \")\n",
    "        search_word = str(word)\n",
    "        url = insta_searching(search_word)\n",
    "        driver.get(url)\n",
    "        time.sleep(8)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "select_first(driver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1000개의 게시글을 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "target = 1000\n",
    "\n",
    "for i in range(target):\n",
    "    try:\n",
    "        data = get_content(driver)\n",
    "        results.append(data)\n",
    "        try:\n",
    "            move_next(driver)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "    except:\n",
    "        time.sleep(2)\n",
    "        try:\n",
    "            move_next(driver)\n",
    "        except:\n",
    "            break\n",
    "\n",
    "print(results[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 크롤링 결과 --> pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(results)\n",
    "result_df.columns = ['content','date','like','place','tags']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 분석에 배제될 단어 입력\n",
    "* 엔터 입력 시 종료됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude = []\n",
    "while True:\n",
    "    ex = input(\"분석에 배제될 단어를 입력하세요(엔터시 종료): \")\n",
    "    if ex == '':\n",
    "        break\n",
    "    else:\n",
    "        exclude.append(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "\n",
    "# 게시글 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = result_df\n",
    "N = len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 좋아요 수 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N):\n",
    "    try:\n",
    "        num = int(docs.iloc[i,2][0].getText())\n",
    "    except:\n",
    "        num = 0\n",
    "    docs.iloc[i,2] = num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 광고성 해시태그 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "remov = []\n",
    "ad = [\"#광고\", \"#유료광고\", \"#제품증정광고\", \"#제품협찬\", \"#협찬\"]\n",
    "\n",
    "for i in range(N):\n",
    "    for j in range(len(docs.iloc[i][4])):\n",
    "        if docs.iloc[i][4][j] in ad:\n",
    "            if i not in remov:\n",
    "                remov.append(i)\n",
    "\n",
    "docs = docs.drop(remov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 도배 게시글 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = len(docs)\n",
    "\n",
    "for i in range(M):\n",
    "    docs.iloc[i][4] = tuple(docs.iloc[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = docs.drop_duplicates(keep = 'first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = len(docs)\n",
    "\n",
    "for i in range(L):\n",
    "    docs.iloc[i][4] = list(docs.iloc[i][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이모지 제거\n",
    "* Co: 이모지 제거된 1차원 리스트\n",
    "* cco : 이모지 제거 및 검색 키워드 유지된 2차원 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmEmoji(input):\n",
    "    return input.encode('utf-8', 'ignore').decode('utf-8')\n",
    "\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "\n",
    "only_BMP_pattern = re.compile(\"[\"\n",
    "        u\"\\U00010000-\\U0010FFFF\"\n",
    "                           \"]+\", flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_list=['☀️','•','❣️','✨','⭐','☕️','❤️','@','☁','♥️','✔','♀️','❣','⊙','×','✅','▪️','⬆️','⚡️','✋','❌','^','⭕️','❗️','▫','〰️','✈️','★','▶','➰']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co = []\n",
    "Co = []\n",
    "\n",
    "for i in range(L):\n",
    "    sen = ''\n",
    "    content = docs.iloc[i,0].split()\n",
    "    for word in content:\n",
    "        rm_emj = rmEmoji(word)\n",
    "        rm_ = emoji_pattern.sub(r'', rm_emj)\n",
    "        rm_again = only_BMP_pattern.sub(r'', rm_)\n",
    "        if '#' not in rm_again:\n",
    "            sen += (rm_again+' ')\n",
    "    co.append(sen) \n",
    "\n",
    "for i in range(len(co)):\n",
    "    content = co[i].split()\n",
    "    sen = ''\n",
    "    for word in content:\n",
    "        for emoji in emoji_list:\n",
    "            if emoji in word:\n",
    "                word = word.replace(emoji,\"\")\n",
    "        sen += (word+' ')     \n",
    "    Co.append(sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_word = {search_word}\n",
    "\n",
    "cco=[]\n",
    "\n",
    "for k in range(len(Co)):\n",
    "    temp = Co[k].split()\n",
    "    cco.append([i for i in temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자가 분석에 배제되길 원하는 단어를 게시글에서 제거\n",
    "* Cco: 분석에 배제되는 단어가 제거된 2차원 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_word2 = set()\n",
    "for i in range(len(cco)):\n",
    "    for j in range(len(cco[i])):\n",
    "        for k in range(len(exclude)):\n",
    "            if exclude[k] in cco[i][j]:\n",
    "                remove_word2.update([cco[i][j]])\n",
    "                \n",
    "Cco =[]\n",
    "for i in range(len(cco)):\n",
    "    temp = []\n",
    "    for j in range(len(cco[i])):\n",
    "        if cco[i][j] not in remove_word2:\n",
    "            temp.append(cco[i][j])\n",
    "    Cco.append(temp) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한글 명사만 추출\n",
    "* 한글 처리기로 KoNLPy의 Okt 사용\n",
    "* pos: 검색 키워드의 위치가 저장됨\n",
    "* cc_kor: 한글만 추출 & 검색 키워드 유지된 2차원 리스트\n",
    "* corp: 한글 명사만 추출 & 검색 키워드 제거된 2차원 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_kor = []\n",
    "for i in range(len(Cco)):\n",
    "    temp = []\n",
    "    for word in Cco[i]:\n",
    "        if isKoreanIncluded(word) == True:      \n",
    "            temp.append(word)\n",
    "    if len(temp) != 0:\n",
    "        cc_kor.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "검색 키워드가 okt.nouns에 의해 분리될 수 있어, nouns 사용 전 미리 키워드를 제거하는 작업. 추후에 복구됨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nosearch_cc_kor =[]\n",
    "pos = []\n",
    "\n",
    "for i in range(len(cc_kor)):\n",
    "    ttt = []\n",
    "    for j in range(len(cc_kor[i])):\n",
    "        if search_word not in cc_kor[i][j]:\n",
    "            ttt.append(cc_kor[i][j])\n",
    "        else:\n",
    "            pos.append(i)\n",
    "            \n",
    "    nosearch_cc_kor.append(ttt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corp = []\n",
    "for i in range(len(nosearch_cc_kor)):\n",
    "    temp_kstr = \"\"\n",
    "    for word in nosearch_cc_kor[i]:\n",
    "        temp_kstr += (word+\" \")\n",
    "    new_kstr = okt.nouns(temp_kstr)\n",
    "    corp.append(new_kstr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 전처리 완료\n",
    "* corpu: 전처리 & 키워드 제거된 2차원 리스트\n",
    "* corpus: 전처리 & 키워드 제거된 1차원 리스트\n",
    "* noremcorp: 전처리 & 검색 키워드 유지된 2차원 리스트\n",
    "* Noremcorpus: 전처리 & 검색 키워드 유지된 1차원 리스트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "명사 추출 후 검색 키워드가 다시 등장할 수 있으니 키워드 한 번 더 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpu = []\n",
    "for i in range(len(corp)):\n",
    "    temp = [i for i in corp[i] if i not in remove_word]\n",
    "    corpu.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus =[]\n",
    "for i in range(len(corpu)):\n",
    "    for j in range(len(corpu[i])):\n",
    "        corpus.append(corpu[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosine similarity 계산을 위해 키워드 복구 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noremcorp = []\n",
    "\n",
    "for i in range(len(corp)):\n",
    "    noremcorp.append(corp[i].copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pos)):\n",
    "    noremcorp[pos[i]].append(search_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Noremcorpus =[]\n",
    "for i in range(len(noremcorp)):\n",
    "    for j in range(len(noremcorp[i])):\n",
    "        Noremcorpus.append(noremcorp[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TfidfVectorizer & TF-IDF 값 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상위 10개 단어 순위 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word embedding: TfidfVectorizer\n",
    "* corpus(전처리 & 키워드 제거된 1차원 리스트)를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_corpus = ' '.join(corpus)\n",
    "joined_corpus = joined_corpus.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_corpus = [joined_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "sp_matrix = vectorizer.fit_transform(tfidf_corpus)\n",
    "\n",
    "word2id = defaultdict(lambda : 0)\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx\n",
    "\n",
    "for i, sent in enumerate(tfidf_corpus):\n",
    "    tfidf_ = ((token, sp_matrix[i, word2id[token]]) for token in sent.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF 값 계산\n",
    "* sorted_tfidf: TF-IDF 값 내림차순 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = dict(tfidf_)\n",
    "sorted_tfidf = sorted(tfidf.items(), key = lambda item: item[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"TF-IDF 값 순위\")\n",
    "print()\n",
    "\n",
    "try:\n",
    "    for i in range(10):\n",
    "        print(i+1,\": \",sorted_tfidf[i][0], round(sorted_tfidf[i][1],5))\n",
    "except:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Word2Vec & cosine similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평균 이상의 유사도를 가진 단어들의 wordcloud 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word embedding: Word2Vec\n",
    "* noremcorp(전처리 & 검색 키워드 유지된 2차원 리스트)를 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(noremcorp, sg = 1, window = 5, min_count = 1)\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 검색 키워드와 매 게시글 단어 간 cosine similarity 및 similarity의 평균값 계산\n",
    "* all_avg: cosine similarity 평균값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "sum_value = 0\n",
    "\n",
    "for i in range(len(noremcorp)):\n",
    "    for j in range(len(noremcorp[i])):\n",
    "        sum_value += model.wv.similarity(search_word, noremcorp[i][j])\n",
    "        count += 1\n",
    "\n",
    "all_avg = sum_value/count\n",
    "print(\"average similarity value:\", all_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 평균 초과의 유사도 값을 갖는 게시글 단어만 추출\n",
    "* similar_noremcorp: 평균 초과의 유사도를 갖는 단어가 저장된 1차원 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_noremcorp=[]\n",
    "\n",
    "for i in range(len(noremcorp)):\n",
    "    for j in range(len(noremcorp[i])):\n",
    "        if model.wv.similarity(search_word, noremcorp[i][j]) > all_avg:\n",
    "            similar_noremcorp.append(noremcorp[i][j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유사도 계산 후 키워드 제거\n",
    "* similar_corpus: 평균 초과의 유사도 & 키워드 제거된 1차원 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_corpus = [i for i in similar_noremcorp if i not in remove_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 유사도 평균 초과 단어들의 wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_best = ' '.join(similar_corpus)\n",
    "similar_best = similar_best.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = FreqDist(similar_best.split())\n",
    "wc = WordCloud(font_path='C:\\\\Windows\\\\Fonts\\\\malgun.ttf', width=3000, height=2000, background_color=\"white\", random_state=0)\n",
    "plt.imshow(wc.generate_from_frequencies(freq))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word2Vec & Topic Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토픽 내 단어별 빈도수 및 토픽 별 wordcloud 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gensim 라이브러리로 LDA 객체 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "\n",
    "os.environ['MALLET_HOME'] = 'C:/Mallet'\n",
    "#c드라이브에 Mallet 폴더에 저장 및 환경변수 설정\n",
    "id2word=corpora.Dictionary(noremcorp)\n",
    "id2word.filter_extremes(no_below = 10)\n",
    "\n",
    "#data word -> 게시글별로\n",
    " #corpus_list_to_list -> 해시태그별로 ?\n",
    "texts = noremcorp #data word\n",
    "corpus=[id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "#C:\\Mallet 변수 값 MALLET_HOME\n",
    "#os.environ.update({'MALLET_HOME':r'C:\\Mallet'}) #Mallet path 지정\n",
    "#C:/Users/me/Desktop/Mallet-202108/bin/mallet\n",
    "#os.environ['MALLET_HOME']='C:/Users/me/Desktop/Mallet-202108/bin/mallet'\n",
    "\n",
    "#지정한 폴더의 mallet 모듈 경로 쓰기 & gensim import\n",
    "mallet_path='C:/Mallet/bin/mallet'\n",
    "ldamallet=gensim.models.wrappers.LdaMallet(mallet_path,corpus=corpus, num_topics=4,id2word=id2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토픽 수를 통해 coherence value를 계산 & 토픽 별 정렬한 coherence value 보이기\n",
    "#### 함수 정의: mallet 모듈을 통한 토픽 format 정의\n",
    "* compute_coherence_values 함수는 오래 걸릴 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=texts, dictionary=id2word, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=4, step=2):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=noremcorp, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "\n",
    "\n",
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=4, limit=21, step=2)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "limit=21; start=4; step=2;\n",
    "x = range(start, limit, step)\n",
    "topic_num = 0\n",
    "count = 0\n",
    "max_coherence = 0\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", cv)\n",
    "    coherence = cv\n",
    "    if coherence >= max_coherence:\n",
    "        max_coherence = coherence\n",
    "        topic_num = m\n",
    "        model_list_num = count   \n",
    "    count = count+1\n",
    "\n",
    "def format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=texts):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    #ldamodel[corpus]: lda_model에 corpus를 넣어 각 토픽 당 확률을 알 수 있음\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num,topn=10)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "    print(type(sent_topics_df))\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    \n",
    "    #이부분 수정\n",
    "    #contents = pd.Series(texts)\n",
    "    #sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    \n",
    "    \n",
    "    sent_topics_df = pd.concat([sent_topics_df, docs['tags']], axis=1)\n",
    "    #,docs['timestamp'],docs['tweet_url'],docs['screen_name'],docs['label'] 얘네 지움\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=cc_kor)\n",
    "\n",
    "# Format\n",
    "df_topic_tweet = df_topic_sents_keywords.reset_index()\n",
    "df_topic_tweet.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Tags']\n",
    "#'Timestamp', 'Tweet_url','Screen_name','label'얘네 지움\n",
    "\n",
    "# Show각 문서에 대한 토픽\n",
    "#df_dominant_topic=df_dominant_topic.sort_values(by=['Dominant_Topic'])\n",
    "#df_topic_tweet\n",
    "\n",
    "\n",
    "# Group top 5 sentences under each topic\n",
    "sent_topics_sorteddf_mallet = pd.DataFrame()\n",
    "\n",
    "sent_topics_outdf_grpd = df_topic_sents_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in sent_topics_outdf_grpd:\n",
    "    sent_topics_sorteddf_mallet = pd.concat([sent_topics_sorteddf_mallet, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(1)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "sent_topics_sorteddf_mallet.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "topic_counts = df_topic_sents_keywords['Dominant_Topic'].value_counts()\n",
    "topic_counts.sort_index(inplace=True)\n",
    "\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "topic_contribution\n",
    "\n",
    "lda_inform = pd.concat([sent_topics_sorteddf_mallet, topic_counts, topic_contribution], axis=1)\n",
    "lda_inform.columns=[\"Topic_Num\", \"Topic_Perc_Contrib\", \"Keywords\", \"Tags\",\"Num_Documents\", \"Perc_Documents\"]\n",
    "#, \"timestamp\", \"tweet_url\",\"screen_name\",\"label\"얘네 지움\n",
    "lda_inform = lda_inform[[\"Topic_Num\",\"Keywords\",\"Num_Documents\",\"Perc_Documents\"]]\n",
    "lda_inform\n",
    "#lda_inform.Topic_Num = lda_inform.Topic_Num.astype(int)\n",
    "lda_inform['Topic_Num'] =lda_inform['Topic_Num'] +1\n",
    "lda_inform.Topic_Num = lda_inform.Topic_Num.astype(str)\n",
    "lda_inform['Topic_Num'] =lda_inform['Topic_Num'].str.split('.').str[0]\n",
    "df_topic_tweet['Dominant_Topic'] =df_topic_tweet['Dominant_Topic'] +1\n",
    "df_topic_tweet.Dominant_Topic = df_topic_tweet.Dominant_Topic.astype(str)\n",
    "df_topic_tweet['Dominant_Topic'] =df_topic_tweet['Dominant_Topic'].str.split('.').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_model = model_list[model_list_num]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "print(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 함수 정의: 적절한 토픽 수 찾기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=4, step=2):\n",
    "\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=noremcorp, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토픽 수에 대한 coherence value를 그래프로 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=texts, start=6, limit=41, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit=41; start=6; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토픽 모델링 및 coherence value를 csv 파일로 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_inform.to_csv (\"C:/Users/me/Desktop/lda_inform.csv\", index = None)\n",
    "lda_inform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LDA 모델의 coherence score 출력\n",
    "* 높을수록 좋다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=noremcorp, dictionary=id2word,coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()      \n",
    "print('\\nCoherence Score: ' , coherence_ldamallet)                                          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토픽 모델링 결과를 pyLDAvis로 시각화 및 html 파일로 저장\n",
    "* 람다 값을 조절하여, 두 기준의 가중치를 변경하여 새로운 키워드를 도출 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.wrappers.ldamallet.malletmodel2ldamodel(ldamallet)\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(model,corpus,id2word)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.save_html(vis,'LDAMallet_TopicData_Visualization.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토픽별 wordcloud 중 4개"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]  # more colors: 'mcolors.XKCD_COLORS'\n",
    "\n",
    "cloud = WordCloud(\n",
    "                  background_color='white',\n",
    "                  width=2500,\n",
    "                  height=1800,\n",
    "                  max_words=10,\n",
    "                  colormap='tab10',\n",
    "                  color_func=lambda *args, **kwargs: cols[i],\n",
    "                  prefer_horizontal=1.0,\n",
    "                   font_path='C:\\\\Windows\\\\Fonts\\\\malgun.ttf' )\n",
    "\n",
    "topics = ldamallet.show_topics(formatted=False)\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10,10), sharex=True, sharey=True)\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    fig.add_subplot(ax)\n",
    "    topic_words = dict(topics[i][1])\n",
    "    cloud.generate_from_frequencies(topic_words, max_font_size=300)\n",
    "    plt.gca().imshow(cloud)\n",
    "    plt.gca().set_title('Topic ' + str(i), fontdict=dict(size=16))\n",
    "    plt.gca().axis('off')\n",
    "\n",
    "\n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.axis('off')\n",
    "plt.margins(x=0, y=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모든 토픽의 wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(background_color='white',\n",
    "        font_path='C:\\\\Windows\\\\Fonts\\\\malgun.ttf')\n",
    "\n",
    "plt.figure(figsize=(30,30))\n",
    "for t in range(ldamallet.num_topics):\n",
    "    plt.subplot(5,4,t+1)\n",
    "    x = dict(ldamallet.show_topic(t,200))\n",
    "    im = wc.generate_from_frequencies(x)\n",
    "    plt.imshow(im)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Topic #\" + str(t))\n",
    "\n",
    "plt.savefig('LDA_wordcloud.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 토픽 별 단어 빈도수를 그래프화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"font.family\"] = \"NanumGothic\"\n",
    "topics = ldamallet.show_topics(formatted=False)\n",
    "data_flat = [w for w_list in noremcorp for w in w_list]\n",
    "counter = Counter(data_flat)\n",
    "\n",
    "out = []\n",
    "for i, topic in topics:\n",
    "    for word, weight in topic:\n",
    "        out.append([word, i , weight, counter[word]])\n",
    "\n",
    "df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "# Plot Word Count and Weights of Topic Keywords\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "    ax_twin = ax.twinx()\n",
    "    #ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "    ax.set_ylabel('Word Count', color=cols[i])\n",
    "    ax_twin.set_ylim(0, 1); ax.set_ylim(0, 500)\n",
    "    ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "    ax.tick_params(axis='y', left=False,)\n",
    "    ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "    ax.legend(loc='upper left'); ax_twin.legend(loc='upper right')\n",
    "    \n",
    "fig.tight_layout(w_pad=2)    \n",
    "fig.suptitle('Word Count of Topic Keywords', fontsize=22, y=1.05)    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 해시태그 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 해시태그만 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdocs = docs.iloc[:,[4]]\n",
    "N = len(hdocs)\n",
    "\n",
    "hdocs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 해시태그의 '#' 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N):\n",
    "    for j in range(len(hdocs.iloc[i,0])):\n",
    "        hdocs.iloc[i,0][j] = hdocs.iloc[i,0][j].replace(\"#\",\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(N):       \n",
    "    string = ' '.join([str(item) for item in hdocs.iloc[i,0]])\n",
    "    hdocs.iloc[i,0] = string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdocs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 이모지 제거\n",
    "* co_: 이모지 제거된 1차원 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ = []\n",
    "for i in range(N):\n",
    "    arr = hdocs.iloc[i,0].split()\n",
    "    c_ += arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co_ = []\n",
    "for i in range(N):\n",
    "    for word in c_:\n",
    "        rm_emj = rmEmoji(word)\n",
    "        rm_ = emoji_pattern.sub(r'', rm_emj)\n",
    "        rm_again = only_BMP_pattern.sub(r'', rm_)\n",
    "        if '#' not in rm_again:\n",
    "            co_.append(rm_again) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in co_:\n",
    "    for emoji in emoji_list:\n",
    "        if emoji in word:\n",
    "            pos = co_.index(word)\n",
    "            co_[pos] = word.replace(emoji,\"\")\n",
    "            \n",
    "    if '' in co_:\n",
    "        co_.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 검색 키워드 제거\n",
    "* cc: 검색 키워드 제거된 1차원 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_word = {search_word}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = [i for i in co_ if i not in remove_word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 소통용 해시태그 제거\n",
    "* CC: 소통용 해시태그 제거된 1차원 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily = ['좋반','좋아요','좋아요반사','좋아요테러','좋반테러','좋테','좋아요그램','선팔','맞팔','선팔하면맞팔','선팔맞팔','맞팔해요','선팔환영','맞팔환영','팔로미','팔로우','팔로우미','팔로','소통', '일상','데일리','디엠환영','디엠','인친']\n",
    "CC = []\n",
    "for i in range(len(cc)):\n",
    "    if cc[i] not in daily:\n",
    "        CC.append(cc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자가 분석에 제거되길 원하는 단어가 포함된 해시태그를 제거\n",
    "* CC: 소통용 해시태그 & 사용자 지정 배제 해시태그 제거된 1차원 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_word3 = set()\n",
    "for i in range(len(CC)):\n",
    "    for j in range(len(exclude)):\n",
    "        if exclude[j] in CC[i]:\n",
    "            remove_word3.update([CC[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC = [i for i in CC if i not in remove_word3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 한글만 추출 및 전처리 완료\n",
    "* cc_kor: 전처리 완료된 해시태그가 저장된 1차원 리스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc_kor_ = []\n",
    "for word in CC:\n",
    "    if isKoreanIncluded(word) == True:      \n",
    "        cc_kor_.append(word)\n",
    "    if '' in cc_kor_:\n",
    "        cc_kor_.remove('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TfidfVectorizer & TF-IDF 값 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 상위 10개 해시태그의 순위 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### word embedding: TfidfVectorizer\n",
    "* cc_kor(전처리 완료된 해시태그가 저장된 1차원 리스트)을 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ccc = ' '.join(cc_kor_)\n",
    "cc_ = ccc.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtag_corpus = [cc_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "sp_matrix = vectorizer.fit_transform(hashtag_corpus)\n",
    "\n",
    "word2id = defaultdict(lambda : 0)\n",
    "for idx, feature in enumerate(vectorizer.get_feature_names()):\n",
    "    word2id[feature] = idx\n",
    "\n",
    "for i, sent in enumerate(hashtag_corpus):\n",
    "    tfidf_ = ((token, sp_matrix[i, word2id[token]]) for token in sent.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF값 계산\n",
    "* sorted_tfidf_: TF-IDF 값 내림차순 정렬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = dict(tfidf_)\n",
    "sorted_tfidf_ = sorted(tfidf.items(), key = lambda item: item[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"TF-IDF 값 순위\")\n",
    "print()\n",
    "try:\n",
    "    for i in range(10):\n",
    "        print(i+1,\": \",sorted_tfidf_[i][0], round(sorted_tfidf_[i][1],5))\n",
    "except:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 해시태그 빈도수에 따른 wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word embedding 없이, 오직 해시태그 자체의 빈도수에 따른 wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = FreqDist(tuple(cc_kor_))\n",
    "wc = WordCloud(font_path='C:\\\\Windows\\\\Fonts\\\\malgun.ttf', width=3000, height=2000, background_color=\"white\", random_state=0)\n",
    "plt.imshow(wc.generate_from_frequencies(freq))\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 부가기능: 좋아요 수에 따른 인기 게시물 순위"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 모든 언어의 상위 10개 게시글 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_like = docs.sort_values(by=docs.columns[2],ascending = False)\n",
    "\n",
    "docs_like.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 부가기능: 많이 언급된 인기 있는 장소"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place = docs.iloc[:,[3]]\n",
    "N = len(place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_list = []\n",
    "for i in range(N):\n",
    "    place_list.append(place.iloc[i,0])\n",
    "    if '' in place_list:\n",
    "        place_list.remove('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_withcount = []\n",
    "for i in place_list:\n",
    "    found = False\n",
    "    count_place = []\n",
    "    count_place.append(i)\n",
    "    count_place.append(place_list.count(i))\n",
    "    for j in range(len(place_withcount)):\n",
    "        if i == place_withcount[j][0]:\n",
    "            found = True\n",
    "    if found == False:\n",
    "        place_withcount.append(count_place)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place_withcount.sort(key = lambda x:-x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    plt.bar(i+1, place_withcount[i][1])\n",
    "plt.xticks(range(len(place_withcount)+1), range(len(place_withcount)+1))\n",
    "plt.show()\n",
    "\n",
    "for i in range(10):\n",
    "    print(i+1,\": \",place_withcount[i][0], \"(\", place_withcount[i][1], \"개)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
